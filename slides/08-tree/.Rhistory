reticulate::repl_python()
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)
# Numerical summaries stratified by the outcome `AHD`.
Hitters %>% tbl_summary()
reticulate::repl_python()
# For reproducibility
set.seed(203)
data_split <- initial_split(
Hitters,
prop = 0.5
)
data_split
Hitters_other <- training(data_split)
dim(Hitters_other)
Hitters_test <- testing(data_split)
dim(Hitters_test)
reticulate::repl_python()
rf_recipe <-
recipe(
Salary ~ .,
data = Hitters_other
) %>%
# # create traditional dummy variables (not necessary for random forest in R)
# step_dummy(all_nominal()) %>%
step_naomit(Salary) %>%
# zero-variance filter
step_zv(all_numeric_predictors()) %>%
# # center and scale numeric data (not necessary for random forest)
# step_normalize(all_numeric_predictors()) %>%
# estimate the means and standard deviations
prep(training = Hitters_other, retain = TRUE)
rf_recipe
rf_mod <-
rand_forest(
mode = "regression",
# Number of predictors randomly sampled in each split
mtry = tune(),
# Number of trees in ensemble
trees = tune()
) %>%
set_engine("ranger")
rf_mod
reticulate::repl_python()
rf_wf <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_mod)
reticulate::repl_python()
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)
# Numerical summaries stratified by the outcome `AHD`.
Hitters %>% tbl_summary()
reticulate::repl_python()
# For reproducibility
set.seed(203)
data_split <- initial_split(
Hitters,
prop = 0.5
)
data_split
Hitters_other <- training(data_split)
dim(Hitters_other)
Hitters_test <- testing(data_split)
dim(Hitters_test)
reticulate::repl_python()
rf_recipe <-
recipe(
Salary ~ .,
data = Hitters_other
) %>%
# # create traditional dummy variables (not necessary for random forest in R)
# step_dummy(all_nominal()) %>%
step_naomit(Salary) %>%
# zero-variance filter
step_zv(all_numeric_predictors()) # %>%
# # center and scale numeric data (not necessary for random forest)
# step_normalize(all_numeric_predictors()) %>%
# estimate the means and standard deviations
# prep(training = Hitters_other, retain = TRUE)
rf_recipe
rf_mod <-
rand_forest(
mode = "regression",
# Number of predictors randomly sampled in each split
mtry = tune(),
# Number of trees in ensemble
trees = tune()
) %>%
set_engine("ranger")
rf_mod
reticulate::repl_python()
rf_wf <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_mod)
rf_wf
rf_recipe <-
recipe(
Salary ~ .,
data = Hitters_other
) %>%
# # create traditional dummy variables (not necessary for random forest in R)
# step_dummy(all_nominal()) %>%
step_naomit(Salary) %>%
# zero-variance filter
step_zv(all_numeric_predictors()) # %>%
# # center and scale numeric data (not necessary for random forest)
# step_normalize(all_numeric_predictors()) %>%
# estimate the means and standard deviations
# prep(training = Hitters_other, retain = TRUE)
rf_recipe
rf_mod <-
rand_forest(
mode = "regression",
# Number of predictors randomly sampled in each split
mtry = tune(),
# Number of trees in ensemble
trees = tune()
) %>%
set_engine("ranger")
rf_mod
rf_wf <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(rf_mod)
rf_wf
param_grid <- grid_regular(
trees(range = c(100L, 300L)),
mtry(range = c(1L, 5L)),
levels = c(3, 5)
)
param_grid
set.seed(203)
folds <- vfold_cv(Hitters_other, v = 5)
folds
rf_fit <- rf_wf %>%
tune_grid(
resamples = folds,
grid = param_grid,
metrics = metric_set(rmse, rsq)
)
rf_fit
rf_fit %>%
collect_metrics() %>%
print(width = Inf) %>%
filter(.metric == "rmse") %>%
mutate(mtry = as.factor(mtry)) %>%
ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
# geom_point() +
geom_line() +
labs(x = "Num. of Trees", y = "CV mse")
rf_fit %>%
show_best("rmse")
rf_fit %>%
show_best(metric = "rmse")
best_rf <- rf_fit %>%
select_best(metric = "rmse")
best_rf
# Final workflow
final_wf <- rf_wf %>%
finalize_workflow(best_rf)
final_wf
# Fit the whole training set, then predict the test cases
final_fit <-
final_wf %>%
last_fit(data_split)
final_fit
# Test metrics
final_fit %>%
collect_metrics()
library(vip)
install.packages("vip")
library(vip)
final_tree <- extract_workflow(final_fit)
final_tree %>%
extract_fit_parsnip() %>%
vip()
reticulate::repl_python()
tree_wf <- workflow() %>%
add_recipe(tree_recipe) %>%
add_model(regtree_mod)
tree_fit %>%
show_best(metric = "rmse")
reticulate::repl_python()
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)
# Numerical summaries stratified by the outcome `AHD`.
Hitters %>% tbl_summary()
Hitters <- Hitters %>% filter(!is.na(Salary)) %>%
select(Salary, Assists,
AtBat, Hits, HmRun,
PutOuts, RBI, Runs, Walks, Years)
reticulate::repl_python()
# For reproducibility
set.seed(203)
data_split <- initial_split(
Hitters,
prop = 0.5
)
data_split
Hitters_other <- training(data_split)
dim(Hitters_other)
Hitters_test <- testing(data_split)
dim(Hitters_test)
reticulate::repl_python()
gb_recipe <-
recipe(
Salary ~ .,
data = Hitters_other
) %>%
# # create traditional dummy variables (not necessary for random forest in R)
# step_dummy(all_nominal()) %>%
step_naomit(Salary) %>%
# zero-variance filter
step_zv(all_numeric_predictors()) # %>%
# # center and scale numeric data (not necessary for random forest)
# step_normalize(all_numeric_predictors()) %>%
# estimate the means and standard deviations
# prep(training = Hitters_other, retain = TRUE)
gb_recipe
gb_mod <-
boost_tree(
mode = "regression",
trees = 1000,
tree_depth = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost")
gb_mod
reticulate::repl_python()
gb_wf <- workflow() %>%
add_recipe(gb_recipe) %>%
add_model(gb_mod)
gb_wf
reticulate::repl_python()
param_grid <- grid_regular(
tree_depth(range = c(1L, 4L)),
learn_rate(range = c(-3, -0.5), trans = log10_trans()),
levels = c(4, 10)
)
param_grid
reticulate::repl_python()
